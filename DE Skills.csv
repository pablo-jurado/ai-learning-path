Contents,,,,,,,
Data Modeling & Querying,"Focuses on understanding, designing, and querying structured data",,,,,,
Data Integration & Transformation,"Covers how data is ingested, transformed, and prepared for use",,,,,,
Distributed Data Processing,Focuses on how to process large datasets at scale,,,,,,
Workflow Orchestration & Automation,"How pipelines are scheduled, monitored, and maintained",,,,,,
Programming & Software Engineering for Data,Software engineering best practices adapted for the data context,,,,,,
Storage Systems & Data Formats,How and where data is stored.,,,,,,
"Data Quality, Observability & Reliability","Focuses on trust, monitoring, and operational excellence.",,,,,,
"Governance, Security & Compliance","Responsible handling of data across access, privacy, and regulations",,,,,,
Project Management,Project planning and execution,,,,,,
,,,,,,,
CHANNEL: Data Engineering,,,,,,,
Subdomain,Skill,Beginner,Novice,Competent,Proficient,Expert,
Data Modeling & Querying,SQL,Understand SQL syntax and can write basic SELECT statements,"Use JOINs, aggregations, and nested queries",Optimize queries with indexes and analyze query plans,Refactor complex queries for maintainability and performance,Design SQL standards and lead query performance tuning at scale,
,Dimensional Modeling,Identify dimensions and facts in a dataset,Build basic star schemas in BI tools,Design robust schemas for reporting across domains,Normalize/denormalize models based on use cases and access patterns,Architect enterprise-wide dimensional models and standards,
,Medallion Modeling,"Can land raw, immutable data in a dedicated ingestion layer and retrieve specific batches by timestamp or file name","Cleanses and normalizes data incrementally into a refined layer, enforcing schema and basic data?quality rules","Integrates and enriches datasets into a conformed analytical layer, resolving keys, managing history, and documenting lineage","Optimizes performance, cost, and reliability end?to?end with tuned storage formats, automated tests, and proactive monitoring","Architects metadata?driven, policy?governed Medallion platforms at enterprise scale and mentors teams on reusable best practices",
,"BCNF, Normalization Forms
(1NF - 5NF)",Identifies repeating groups and converts unstructured tables to First Normal Form (1NF) by enforcing atomic columns and unique row identifiers,"Eliminates partial and transitive dependencies to achieve Second and Third Normal Forms (2NF,?3NF), defining clear primary–foreign?key relationships","Decomposes schemas into Boyce?Codd and Fourth Normal Forms (BCNF,?4NF), resolving anomaly?causing functional and multi?valued dependencies","Applies or relaxes up to Fifth Normal Form (5NF) based on query patterns, articulating performance vs. integrity trade?offs and documenting rationale","Establishes organization?wide normalization standards, automates dependency analysis, and mentors teams on designing scalable, anomaly?free relational models",
,Schema Design,Choose appropriate data types and constraints,Create normalized schemas aligned with data usage,Design schemas for scalability and data integration,"Manage schema evolution, compatibility, and backward/forward changes",Define enterprise-wide schema governance processes,
,Data Discovery,Document datasets and add tags/descriptions,Maintain dataset lineage and usage examples,Collaborate with teams to define ownership and business definitions,"Implement organization-wide discovery, governance, and metadata strategy",Lead data workshops with execs and relevant stakeholders,
Data Integration & Transformation,ETL/ELT Design,Describe ETL/ELT flow stages and tools,Build basic pipelines using tools like dbt or Airflow,Design reusable pipeline components with testing,"Handle dependencies, failures, and recovery in pipeline frameworks",Architect end-to-end data movement systems across platforms,
,Python for Data Engineering,Use pandas to clean and merge small datasets,Build scripts using functions and error handling,Design reusable modules for pipeline tasks,Write performant code with memory profiling and logging,"Configure spark environments, practice paralellism and partitioning, and balances cost, security, governance, and performance trade?offs when choosing file formats, clusters, and orchestration patterns",
,Shell Scripting,Create basic scripts for core file and directory operations,"Employ variables, loops, and conditionals; schedule execution with the operating system’s native job scheduler","Develop modular, reusable scripts with robust logging and error handling",Integrate scripts with orchestration frameworks or cloud command?line interfaces to automate infrastructure and deployments,"Architect shell?based boot?strap, health?check, and monitoring utilities for production environments",
,Data Masking & Anonymization,Identify sensitive fields and masking needs,Apply column-level masking using tools or scripts,Implement masking for multiple formats and systems,Build re-identification-safe anonymization pipelines,Standardize privacy-preserving transformations org-wide,
,Stored Procedures,Execute simple stored procedures with parameters,Write basic procedures with control flow and error handling,"Build procedures with logic, transactions, and optimization","Design modular, reusable procedures for pipelines","Define standards, refactor legacy code, and understand sys tables and meta data and engine complexities",
Distributed Data Processing,Apache Spark,Run simple PySpark transformations,"Use Spark SQL, caching, and partitioning",Tune jobs using memory and execution metrics,Optimize DAG execution and Spark configurations,Architect distributed compute strategies across clusters,
,Kafka Streams,"Understand producers, topics, and consumers",Stream and transform messages using Kafka tools,Ensure fault tolerance and message ordering,Manage schema evolution and compaction strategies,Design low-latency streaming architectures at scale,
,Performance Tuning,Identify performance bottlenecks in basic code,Profile Spark or Python jobs using logs and metrics,"Use partitioning, batching, and caching to optimize throughput","Implement job retries, resource scaling, and backpressure handling",Tune end-to-end data platforms with cost/performance tradeoffs,
,Resource Management,Monitor job memory and CPU usage,Set limits and quotas for jobs and containers,Balance load across clusters or pipeline stages,Automate autoscaling policies in orchestration tools,Optimize cost and SLA tradeoffs with resource budgets,
,Compute Engines,"Runs individual jobs on a single?node or default cluster configuration, setting only basic CPU and memory parameters","Monitors resource metrics and adjusts executor / worker sizing, parallelism, and basic autoscaling policies to keep jobs stable","Designs distributed workflows that exploit shuffling, caching, and data locality, choosing the right compute engine (Spark, Dask, Flink, etc.) for each workload","Tunes multi?tenant clusters for mixed batch?and?stream processing, implements checkpointing and fault recovery, and enforces security, quota, and cost?governance rules","Architects adaptive, cloud?agnostic compute platforms that dynamically allocate resources across heterogeneous engines, automating workload placement and mentoring teams on performance engineering best practices",
Workflow Orchestration & Automation,DAG Scheduling,Lists task dependencies and produces a simple topological (topographic) order to run steps one after another,Uses basic graph traversals like DFS or BFS to confirm the DAG is acyclic and derive an initial run sequence,"Implements formal topological?sort algorithms (e.g., Kahn’s) to surface parallel branches and generate concurrency?safe execution plans","Optimizes schedules via critical?path analysis and longest?path heuristics, reordering tasks to maximize resource usage while honoring dependencies","Designs adaptive DAG schedulers that dynamically re?prioritize nodes with algorithmic strategies (leveled BFS, priority queues, hybrid DFS/BFS) to balance load and guarantee deterministic results at scale",
,CI/CD for Pipelines,Push and pull pipeline code using Git,Add linting and simple tests to CI workflows,Deploy pipelines through automation pipelines. Understand topographics,Use secrets and multi-environment configurations,Architect robust CI/CD workflows for pipeline stacks. Understand application of BFS and DFS and when to apply them,
,Monitoring & Alerting,Read logs and detect job failures,Configure alerts on DAG failures and SLAs,Build basic dashboard for pipeline health,"Integrate observability tooling (e.g., Prometheus, Grafana)",Implement unified monitoring across orchestration layers,
,Dependency Management,Order basic tasks and define static dependencies,Use conditional branching and sensors in DAGs,"Handle retries, backfills, and downstream effects",Manage inter-DAG and external task dependencies,Design global DAG dependency and alert frameworks,
Programming & Software Engineering for Data,Git & Version Control,Clone repos and push commits,Use feature branches and resolve merge conflicts,Apply semantic versioning and code reviews,Enforce repo structure and commit standards,Design branching strategy and enforce across teams,
,Unit & Integration Testing,Write tests for functions and expected output,Use mocks and fixtures for simple data tests,Test integration across pipeline components,Automate testing in CI/CD pipelines,Develop data test frameworks with schema and value checks,
,Modular Design,"Craft and consistently reuse small, single?responsibility functions to speed personal development and demonstrate quick wins for clients","Group related functions into clear modules or installable packages so teammates at the client site can import, configure, and extend them with minimal friction","Refactor end?to?end pipelines into unit?tested, parameter?driven components that isolate client?specific logic from generic processing steps","Publish reusable data libraries and reference processes to the consulting project’s shared repo—complete with README examples, versioning, and basic CI—so multiple squads can adopt a common approach","Architect a plug?in?based pipeline framework with documented extension points that lets future consulting teams drop in new sources, transforms, or sinks without rewriting core orchestration or quality controls",
,Containerization,Build container images for local development and testing,Run and orchestrate multi?container stacks locally to support development workflows,Package application or data pipelines into containers for consistent deployment across environments,Deploy and scale containers with a cluster?level orchestration platform,"Govern image registries, curate base images, and enforce container security and compliance",
Storage Systems & Data Formats,Cloud Storage,Upload and download files using CLI or SDK,Configure permissions and lifecycle rules,Automate data storage with versioning and encryption,Manage regional replication and failover,"Design secure, cost-effective cloud storage systems",
,Data Warehouses,Query and write to data warehouses,Design partitioning and clustering strategies,Manage cost and performance tradeoffs,Optimize metadata configs and materialized views,"Architect cross-region, scalable warehouse systems",
,File Formats,"Read and write CSV, JSON, Parquet",Choose efficient formats for data use,Tune compression and encoding settings,Benchmark file performance across workflows,Standardize format use and storage strategy org-wide,
,Partitioning & Bucketing,Query partitioned datasets by filter,"Design partitioning strategies (e.g., date, region)",Optimize bucket sizes and split thresholds,Manage file layout for parallelism and cost,Implement partition/bucket strategies across domains,
"Data Quality, Observability & Reliability",Data Quality Validation,Run row-level or column-level checks,Use validation libraries (e.g. Great Expectations),Integrate tests into orchestration workflows,Automate schema and value checks at scale,Create shared quality definitions and test registries,
,Anomaly Detection,Identify outliers using visual summaries,Use thresholds and statistical checks,Detect schema and distribution drift,Set up alerting on model and data health,Build ML-based anomaly detection on pipelines,
,Logging,Add print/debug statements to scripts,Use structured logging for events,Write logs to files or systems like CloudWatch,Centralize logs and monitor patterns,Define logging and alerting strategy for observability stack,
,SLA Management,Track pipeline run time and failures,Configure task-level SLAs and alerts,Analyze trends in SLA misses,Report SLA violations and mitigations,Automate SLA enforcement and escalation policies,
"Governance, Security & Compliance",IAM & Access Control,Access data using role credentials,Apply fine-grained permissions on datasets,Audit permissions and secure credentials,Implement RBAC/ABAC across services,Design and manage org-wide data access controls,
,Encryption,Understand in-transit and at-rest encryption,Use managed encryption services like KMS,Encrypt files programmatically,Rotate keys and manage secure access,Design encryption policy across data domains,
,Metadata Management,Read table metadata and column info,Document metadata manually,"Automate metadata population (e.g., via ingestion)",Ensure metadata accuracy and lineage tracking,Build enterprise metadata pipelines and governance,
,Compliance Standards,"Understand common regulations (GDPR, HIPAA)",Apply tagging or masking to PII fields,Document controls and audits for compliance,Monitor compliance with automated checks,Work with Security Stakeholders / Legal to identify potential crises areas,
DE Project Management,Project Management,"Captures high?level objectives and lists the basic ETL tasks, data sources, and success criteria in a simple checklist or spreadsheet.","Breaks work into sprints or milestones, estimates effort, and tracks task status while flagging obvious data?quality or dependency risks.","Produces a detailed project plan that maps data flows, resource needs, and cross?team hand?offs, complete with timelines, risk log, and stakeholder communication cadence.","Integrates architecture decisions, capacity?/?cost projections, compliance checkpoints, and CI/CD gates into the roadmap, continuously re?forecasting based on velocity and metrics.","Orchestrates multi?program portfolios that align data?platform evolution with business OKRs, sequencing parallel workstreams, governing scope change, and defining measurable value KPIs.",
,,,,,,,
,,,,,,,
,,,,,,,
,,,,,,,
